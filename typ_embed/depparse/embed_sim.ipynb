{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dep_model import *\n",
    "from dep_data_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace()\n",
    "args.base_path = './'\n",
    "args.data_path = '../datasets/UD_English-EWT'\n",
    "args.lang = 'en'\n",
    "train_filename = 'en_ewt-ud-train.conllu'\n",
    "valid_filename = 'en_ewt-ud-dev.conllu'\n",
    "test_filename = 'en_ewt-ud-test.conllu'\n",
    "args.shuffle = False\n",
    "\n",
    "args.word_embed_size = 100\n",
    "args.pos_embed_size = 100\n",
    "args.attention_hidden_size = 200\n",
    "args.lm_model_name = 'bert-base-uncased'\n",
    "args.encoder = 'lm'\n",
    "args.lstm_hidden_size = 400\n",
    "args.lstm_layers = 3\n",
    "args.dropout = 0.33\n",
    "args.lm_layer = 8\n",
    "args.scale = 0\n",
    "args.typological = False\n",
    "args.typ_embed_size = 32\n",
    "args.num_typ_features = 289\n",
    "args.typ_encode = None\n",
    "args.fine_tune = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, vocab_dict, pos_dict, label_dict = dep_data_loaders(args, train_filename, valid_filename, test_filename)\n",
    "pad_index = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_classifier = BiaffineDependencyModel(n_words = len(vocab_dict), n_pos = len(pos_dict), n_rels = len(label_dict), word_embed_size = args.word_embed_size, pos_embed_size = args.pos_embed_size, lstm_hidden_size = args.lstm_hidden_size, encoder = args.encoder, lstm_layers = args.lstm_layers, \n",
    "    lm_model_name = args.lm_model_name, dropout = args.dropout, n_lm_layer = args.lm_layer, n_arc_mlp = 500, n_rel_mlp = 100, scale = args.scale, pad_index = pad_index, \n",
    "    unk_index = 0, typological = args.typological, typ_embed_size = args.typ_embed_size, num_typ_features = args.num_typ_features, \n",
    "    typ_encode = args.typ_encode, attention_hidden_size = args.attention_hidden_size, fine_tune = args.fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(args.base_path, 'saved_models', 'lm_dep_model.pt')\n",
    "lm_classifier.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "with open('common_words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "non_typ_embeddings  = {}\n",
    "for w in lines:\n",
    "    tokens = tokenizer.encode(w.strip('\\n'), return_tensors = 'pt')\n",
    "    non_typ_embeddings[w] = torch.mean(lm_classifier.encode.lm(tokens)[:, 1:-1, :], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.typological = True\n",
    "args.typ_encode = 'add_att'\n",
    "args.typ_feature = 'syntax_knn+phonology_knn+inventory_knn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_lm_classifier = BiaffineDependencyModel(n_words = len(vocab_dict), n_pos = len(pos_dict), n_rels = len(label_dict), word_embed_size = args.word_embed_size, pos_embed_size = args.pos_embed_size, lstm_hidden_size = args.lstm_hidden_size, encoder = args.encoder, lstm_layers = args.lstm_layers, \n",
    "    lm_model_name = args.lm_model_name, dropout = args.dropout, n_lm_layer = args.lm_layer, n_arc_mlp = 500, n_rel_mlp = 100, scale = args.scale, pad_index = pad_index, \n",
    "    unk_index = 0, typological = args.typological, typ_embed_size = args.typ_embed_size, num_typ_features = args.num_typ_features, \n",
    "    typ_encode = args.typ_encode, attention_hidden_size = args.attention_hidden_size, fine_tune = args.fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_embeddings = {}\n",
    "typ_embed = typ_lm_classifier.encode.typ(lang = 'en', typ_feature = args.typ_feature, device = 'cpu')\n",
    "for w in lines:\n",
    "    tokens = tokenizer.encode(w.strip('\\n'), return_tensors = 'pt')\n",
    "    embed = typ_lm_classifier.encode.lm(tokens)\n",
    "    embed = torch.mean(embed[:, 1:-1, :], dim = 1)\n",
    "    output = typ_lm_classifier.encode.attention.forward(typ_embed, embed.squeeze(0))\n",
    "    typ_embeddings[w] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_embeddings = torch.cat(list(non_typ_embeddings.vals())).detach().cpu.numpy()\n",
    "typ_embeddings = torch.cat(list(typ_embeddings.vals())).detach().cpu().numpy()\n",
    "similarities = np.matmul(typ_embeddings, norm_embeddings.transpose())\n",
    "lines = map(str.strip('\\n'), lines)\n",
    "words = np.array(lines)\n",
    "sorted_indices = words.argsort()\n",
    "result = similarities[sorted_indices][:, sorted_indices]\n",
    "plt.title('Cosine similarity between non-typological and typological BERT embeddings')\n",
    "plt.xlabel('non-typological embedding')\n",
    "plt.ylabel('typological embedding')\n",
    "plt.imshow(1-result, cmap = 'hot', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_embed = non_typ_embeddings['happy'].detach().cpu().numpy()\n",
    "happy_sim_dict = {}\n",
    "for w in non_typ_embeddings:\n",
    "    if w == 'happy':\n",
    "        continue\n",
    "    word_embed = non_typ_embeddings[w].detach().cpu().numpy()\n",
    "    happy_sim_dict[w] = np.matmul(happy_embed, word_embed.transpose())\n",
    "happy_sim_pairs = sorted(happy_sim_dict.items(), key = lambda item: item[1])\n",
    "print(happy_sim_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_happy_embed = typ_embeddings['happy'].detach().cpu().numpy()\n",
    "typ_happy_sim_dict = {}\n",
    "for w in typ_embeddings:\n",
    "    if w == 'happy':\n",
    "        continue\n",
    "    word_embed = typ_embeddings[w].detach().cpu().numpy()\n",
    "    typ_happy_sim_dict[w] = np.matmul(happy_embed, word_embed.transpose())\n",
    "typ_happy_sim_pairs = sorted(typ_happy_sim_dict.items(), key = lambda item: item[1])\n",
    "print(typ_happy_sim_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[0] for x in happy_sim_pairs]\n",
    "scores = [x[1] for x in happy_sim_pairs]\n",
    "x_pos = [i for i,_ in enumerate(words)]\n",
    "plt.bar(x_pos, scores, color = 'green')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Similarity')\n",
    "plt.title('Similarity to happy and most common english words using frozen embeddings')\n",
    "plt.xticks(x_pos, words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[0] for x in typ_happy_sim_pairs]\n",
    "scores = [x[1] for x in typ_happy_sim_pairs]\n",
    "x_pos = [i for i,_ in enumerate(words)]\n",
    "plt.bar(x_pos, scores, color = 'green')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Similarity')\n",
    "plt.title('Similarity to happy and most common english words using typological embeddings')\n",
    "plt.xticks(x_pos, words)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d815e1d09b40d1f924a875f363a97e8bffbf6165f0ee10bc8fc7a4b60f3284"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
